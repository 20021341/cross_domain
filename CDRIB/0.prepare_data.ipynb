{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data 1 info : 1956059 53945 4195275\n",
      "raw_data 2 info : 780447 17971 1223805\n",
      "common user num 6011\n",
      "total user num 118980\n",
      "after filter_data1 info : 106522 41995 1385458 0.000310\n",
      "after filter_data2 info : 18469 8866 163874 0.001001\n",
      "after common_data1 info : 6011 28855 179185 0.001033\n",
      "after common_data2 info : 6011 8706 75944 0.001451\n",
      "write data finished!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def pprint(str_, f):\n",
    "    print(str_)\n",
    "    print(str_, end='\\n', file=f)\n",
    "\n",
    "\n",
    "def filter_data(filePath, threshold=10):\n",
    "    data = []\n",
    "    ratings = pd.read_csv(filePath, delimiter=\",\", encoding=\"latin1\")\n",
    "    ratings.columns = ['userId', 'itemId', 'Rating', 'timesteamp']\n",
    "\n",
    "    rate_size_dic_i = ratings.groupby('itemId').size()\n",
    "    # choosed_index_del_i = rate_size_dic_i.index[rate_size_dic_i < 10]\n",
    "    choosed_index_del_i = rate_size_dic_i.index[rate_size_dic_i < threshold]\n",
    "    ratings = ratings[~ratings['itemId'].isin(list(choosed_index_del_i))] # item freq more than 10\n",
    "\n",
    "    user_unique = list(ratings['userId'].unique())\n",
    "    movie_unique = list(ratings['itemId'].unique())\n",
    "\n",
    "    u = len(user_unique)\n",
    "    i = len(movie_unique)\n",
    "    rating_num = len(ratings)\n",
    "    return u, i, rating_num, user_unique, ratings\n",
    "\n",
    "\n",
    "def get_min_group_size(ratings):\n",
    "    rate_size_dic_u = ratings.groupby('userId').size()\n",
    "    return min(rate_size_dic_u)\n",
    "\n",
    "\n",
    "def my_reindex_data(ratings1, dic_u=None):\n",
    "    data = []\n",
    "    if dic_u is None:\n",
    "        user_unique = list(ratings1['userId'].unique())\n",
    "        user_index = list(range(0, len(user_unique)))\n",
    "        dic_u = dict(zip(user_unique, user_index))\n",
    "    movie_unique1 = list(ratings1['itemId'].unique())\n",
    "    movie_index1 = list(range(0, len(movie_unique1)))\n",
    "    dic_m1 = dict(zip(movie_unique1, movie_index1))\n",
    "    for element in ratings1.values:\n",
    "        data.append((dic_u[element[0]], dic_m1[element[1]], 1))\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "    return data, dic_u\n",
    "\n",
    "def reindex_data(ratings1, dic_u=None,dic_v=None):\n",
    "    data = []\n",
    "    if dic_u is None:\n",
    "        user_unique = list(ratings1['userId'].unique())\n",
    "        user_index = list(range(0, len(user_unique)))\n",
    "        dic_u = dict(zip(user_unique, user_index))\n",
    "    if dic_v is None:\n",
    "        movie_unique1 = list(ratings1['itemId'].unique())\n",
    "        movie_index1 = list(range(0, len(movie_unique1)))\n",
    "        dic_v = dict(zip(movie_unique1, movie_index1))\n",
    "    for element in ratings1.values:\n",
    "        data.append((dic_u[element[0]], dic_v[element[1]], 1))\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "    return data, dic_u\n",
    "\n",
    "\n",
    "def get_common_data(data1, data2, common_user):\n",
    "    rating_new_1 = data1[data1['userId'].isin(common_user)]\n",
    "    rating_new_2 = data2[data2['userId'].isin(common_user)]\n",
    "    return rating_new_1, rating_new_2\n",
    "\n",
    "\n",
    "def get_unique_lenth(ratings):\n",
    "    r_n = len(ratings)\n",
    "    user_unique = list(ratings['userId'].unique())\n",
    "    movie_unique = list(ratings['itemId'].unique())\n",
    "    u = len(user_unique)\n",
    "    i = len(movie_unique)\n",
    "    return u, i, r_n\n",
    "\n",
    "\n",
    "def filter_user(ratings1, ratings2):\n",
    "    rate_size_dic_u1 = ratings1.groupby('userId').size()\n",
    "    rate_size_dic_u2 = ratings2.groupby('userId').size()\n",
    "    choosed_index_del_u1 = rate_size_dic_u1.index[rate_size_dic_u1 < 5]\n",
    "    choosed_index_del_u2 = rate_size_dic_u2.index[rate_size_dic_u2 < 5]\n",
    "    ratings1 = ratings1[~ratings1['userId'].isin(list(choosed_index_del_u1) + list(choosed_index_del_u2))]\n",
    "    ratings2 = ratings2[~ratings2['userId'].isin(list(choosed_index_del_u1) + list(choosed_index_del_u2))]\n",
    "    return ratings1, ratings2\n",
    "\n",
    "def filter_item(ratings1, ratings2):\n",
    "    rate_size_dic_u1 = ratings1.groupby('itemId').size()\n",
    "    rate_size_dic_u2 = ratings2.groupby('itemId').size()\n",
    "    choosed_index_del_u1 = rate_size_dic_u1.index[rate_size_dic_u1 < 5]\n",
    "    choosed_index_del_u2 = rate_size_dic_u2.index[rate_size_dic_u2 < 5]\n",
    "    ratings1 = ratings1[~ratings1['itemId'].isin(list(choosed_index_del_u1) + list(choosed_index_del_u2))]\n",
    "    ratings2 = ratings2[~ratings2['itemId'].isin(list(choosed_index_del_u1) + list(choosed_index_del_u2))]\n",
    "    return ratings1, ratings2\n",
    "\n",
    "\n",
    "def write_to_txt(data, file):\n",
    "    f = open(file, 'w+')\n",
    "    for i in data:\n",
    "        line = '\\t'.join([str(x) for x in i]) + '\\n'\n",
    "        f.write(line)\n",
    "    f.close\n",
    "\n",
    "\n",
    "def get_common_user(data1, data2):\n",
    "    common_user = list(set(data1).intersection(set(data2)))\n",
    "    return len(common_user), common_user\n",
    "\n",
    "def get_total_user(data1, data2):\n",
    "    total_user = list(set(data1).union(set(data2)))\n",
    "    return len(total_user), total_user\n",
    "\n",
    "\n",
    "# cloth sport\n",
    "# cell electronic\n",
    "# game video\n",
    "# cd movie\n",
    "# music instrument\n",
    "\n",
    "raw_path = '/home/hadh2/projects/cross_domain/cross_domain/CDRIB/dataset/raw_data'\n",
    "data_name_s = 'movie'\n",
    "data_name_t = 'video'\n",
    "save_path = '/home/hadh2/projects/cross_domain/cross_domain/CDRIB/dataset/generated_data'\n",
    "\n",
    "save_path_s = os.path.join(save_path, data_name_s + '_' + data_name_t)\n",
    "save_path_t = os.path.join(save_path, data_name_t + '_' + data_name_s)\n",
    "if not os.path.exists(save_path_s):\n",
    "    os.makedirs(save_path_s)\n",
    "if not os.path.exists(save_path_t):\n",
    "    os.makedirs(save_path_t)\n",
    "\n",
    "data_dic = {'sport': 'ratings_Sports_and_Outdoors', 'electronic': 'ratings_Electronics',\n",
    "            'cloth': 'ratings_Clothing_Shoes_and_Jewelry', 'cell': 'ratings_Cell_Phones_and_Accessories',\n",
    "            'instrument': 'ratings_Musical_Instruments', 'game': 'ratings_Toys_and_Games', 'video': 'ratings_Video_Games',\n",
    "            'book':'ratings_Books', 'movie':'ratings_Movies_and_TV', 'cd': 'ratings_CDs_and_Vinyl', 'music':'ratings_Digital_Music'}\n",
    "\n",
    "\n",
    "filepath1 = os.path.join(raw_path, data_dic[data_name_s] + '.csv')\n",
    "filepath2 = os.path.join(raw_path, data_dic[data_name_t] + '.csv')\n",
    "save_file1 = os.path.join(save_path_s, 'common_new_reindex.txt')\n",
    "save_file2 = os.path.join(save_path_t, 'common_new_reindex.txt')\n",
    "save_file3 = os.path.join(save_path_s, 'new_reindex.txt')\n",
    "save_file4 = os.path.join(save_path_t, 'new_reindex.txt')\n",
    "source_info_path = os.path.join(save_path_s, '%s_%s_data_info.txt' % (data_name_s, data_name_t))\n",
    "target_info_path = os.path.join(save_path_t, '%s_%s_data_info.txt' % (data_name_t, data_name_s))\n",
    "source_info_file = open(source_info_path, 'w+')\n",
    "target_info_file = open(target_info_path, 'w+')\n",
    "u_num, i_num, r_num, user_unique, data = filter_data(filepath1) # item<10\n",
    "u_num2, i_num2, r_num2, user_unique2, data2 = filter_data(filepath2) # item<10\n",
    "\n",
    "\n",
    "data, data2 = filter_user(data, data2) # del overlap user < 5\n",
    "data, data2 = filter_item(data, data2) # del overlap item < 5\n",
    "user_unique = list(data['userId'].unique())\n",
    "user_unique2 = list(data2['userId'].unique())\n",
    "item_unique = list(data['itemId'].unique())\n",
    "item_unique2 = list(data2['itemId'].unique())\n",
    "\n",
    "\n",
    "\n",
    "c_n, common_user = get_common_user(user_unique, user_unique2)\n",
    "t_n, total_user = get_total_user(user_unique, user_unique2)\n",
    "pprint('raw_data 1 info : %d %d %d' % (u_num, i_num, r_num), source_info_file)\n",
    "pprint('raw_data 2 info : %d %d %d' % (u_num2, i_num2, r_num2), source_info_file)\n",
    "pprint('common user num %d' % c_n, source_info_file)\n",
    "pprint('total user num %d' % t_n, source_info_file)\n",
    "\n",
    "\n",
    "new_data_1, new_data_2 = get_common_data(data, data2, common_user) # overlap\n",
    "\n",
    "\n",
    "u, i, r = get_unique_lenth(data)\n",
    "u2, i2, r2 = get_unique_lenth(data2)\n",
    "pprint('after filter_data1 info : %d %d %d %.6f' % (u, i, r, r / (u * i)), source_info_file)\n",
    "pprint('after filter_data2 info : %d %d %d %.6f' % (u2, i2, r2, r2 / (u2 * i2)), source_info_file)\n",
    "\n",
    "u, i, r = get_unique_lenth(new_data_1)\n",
    "u2, i2, r2 = get_unique_lenth(new_data_2)\n",
    "pprint('after common_data1 info : %d %d %d %.6f' % (u, i, r, r / (u * i)), source_info_file)\n",
    "pprint('after common_data2 info : %d %d %d %.6f' % (u2, i2, r2, r2 / (u2 * i2)), source_info_file)\n",
    "\n",
    "\n",
    "user_index = list(range(0, len(total_user)))\n",
    "dic_u = dict(zip(total_user, user_index))\n",
    "\n",
    "item_index1 = list(range(0, len(item_unique)))\n",
    "dic_v1 = dict(zip(item_unique, item_index1))\n",
    "\n",
    "item_index2 = list(range(0, len(item_unique2)))\n",
    "dic_v2 = dict(zip(item_unique2, item_index2))\n",
    "\n",
    "common_data1, dic_u = reindex_data(new_data_1,dic_u,dic_v1)\n",
    "common_data2, dic_u2 = reindex_data(new_data_2, dic_u,dic_v2)\n",
    "write_to_txt(common_data1, save_file1)\n",
    "write_to_txt(common_data2, save_file2)\n",
    "\n",
    "data1, dic_u = reindex_data(data, dic_u, dic_v1)\n",
    "data2, dic_u2 = reindex_data(data2, dic_u, dic_v2)\n",
    "write_to_txt(data1, save_file3)\n",
    "write_to_txt(data2, save_file4)\n",
    "\n",
    "pprint('write data finished!', source_info_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106522\n",
      "18469\n",
      "41995\n",
      "8866\n",
      "Train 1364788, valid 10363, test 10307\n",
      "sss 41994\n",
      "Train 155867, valid 3962, test 4045\n",
      "sss 8865\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def read_dataset(file_name):\n",
    "    data = {}\n",
    "    with open(file_name,\"r\",encoding=\"utf-8\") as fr:\n",
    "        for line in fr:\n",
    "            user, item, score = line.strip().split(\"\\t\")\n",
    "            if user not in data.keys():\n",
    "                data[user] = [item]\n",
    "            else :\n",
    "                data[user].append(item)\n",
    "    return data\n",
    "\n",
    "def create_user_dict(user, data):\n",
    "    user = copy.deepcopy(user)\n",
    "    for u in data.keys():\n",
    "        if u not in user.keys():\n",
    "            user[u] = len(user)\n",
    "    return user\n",
    "\n",
    "def create_item_dict(item, data):\n",
    "    for u in data.keys():\n",
    "        for i in data[u]:\n",
    "            if i not in item.keys():\n",
    "                item[i] = len(item)\n",
    "    return item\n",
    "\n",
    "\n",
    "def ensure_dir(d, verbose=True):\n",
    "    if not os.path.exists(d):\n",
    "        if verbose:\n",
    "            print(\"Directory {} do not exist; creating...\".format(d))\n",
    "        os.makedirs(d)\n",
    "\n",
    "def generate_train_valid_test(file, total, common, user, item, choose):\n",
    "    train_file = file + \"train.txt\"\n",
    "    valid_file = file + \"valid.txt\"\n",
    "    test_file = file + \"test.txt\"\n",
    "\n",
    "    #source_total_data, source_common_data, source_user, source_item, 0\n",
    "    ## total: dict user total: list[item]\n",
    "    ## common: dict user common: list[item]\n",
    "    ## user: user mapping dict from index in file reindex.txt to index of that domain only\n",
    "    ## item: same\n",
    "    ## chose: 0 is source data, 1 is target data\n",
    "    itemx = 0\n",
    "\n",
    "    L = int(len(common) * 0.1)\n",
    "    LL = L * 9\n",
    "    LLL = L * 8\n",
    "\n",
    "    train_number = 0\n",
    "    valid_number = 0\n",
    "    test_number = 0\n",
    "    with codecs.open(train_file,\"w\",encoding=\"utf-8\") as fw:\n",
    "        with codecs.open(test_file, \"w\", encoding=\"utf-8\") as fw2:\n",
    "            with codecs.open(valid_file, \"w\", encoding=\"utf-8\") as fw3:\n",
    "                for da in total: #da is the old user index from file reindex.txt\n",
    "                    if choose == 0 and user[da] >= LL and user[da] < len(common): \n",
    "                        #10% last user of common user is used as valid or test data (random)\n",
    "                        for i in total[da]: # total[da] is the item list of old user index\n",
    "                            itemx = max(itemx, int(item[i]))\n",
    "                            if random.randint(0,1):\n",
    "                                test_number+=1\n",
    "                                fw2.write(str(user[da]) + \"\\t\" + str(item[i]) + \"\\n\") # cold-start user for test\n",
    "                            else :\n",
    "                                valid_number+=1\n",
    "                                fw3.write(str(user[da]) + \"\\t\" + str(item[i]) + \"\\n\")  # cold-start user for valid\n",
    "                    elif choose == 1 and user[da] >= LLL and user[da] < LL:\n",
    "                        for i in total[da]:\n",
    "                            itemx = max(itemx, int(item[i]))\n",
    "                            if random.randint(0, 1):\n",
    "                                test_number += 1\n",
    "                                fw2.write(str(user[da]) + \"\\t\" + str(item[i]) + \"\\n\")  # cold-start user for test\n",
    "                            else:\n",
    "                                valid_number += 1\n",
    "                                fw3.write(str(user[da]) + \"\\t\" + str(item[i]) + \"\\n\")  # cold-start user for valid\n",
    "                    else:\n",
    "                        for i in total[da]:\n",
    "                            train_number += 1\n",
    "                            itemx = max(itemx, int(item[i]))\n",
    "                            fw.write(str(user[da])+\"\\t\"+str(item[i])+\"\\n\")\n",
    "\n",
    "    print(\"Train {}, valid {}, test {}\".format(train_number, valid_number, test_number))\n",
    "    print(\"sss\", itemx)\n",
    "\n",
    "\n",
    "source = \"movie\"\n",
    "target = \"video\"\n",
    "f1 = '/home/hadh2/projects/cross_domain/cross_domain/CDRIB/dataset/generated_data' + '/' + source + \"_\" + target + \"/\"\n",
    "f2 = '/home/hadh2/projects/cross_domain/cross_domain/CDRIB/dataset/generated_data' + '/' + target + \"_\" + source + \"/\"\n",
    "\n",
    "source_common_data = read_dataset(f1 + \"common_new_reindex.txt\") # same users data\n",
    "target_common_data = read_dataset(f2 + \"common_new_reindex.txt\")\n",
    "\n",
    "user_dict = {} # re-index\n",
    "source_item = {}\n",
    "target_item = {}\n",
    "if len(source_common_data) == len(target_common_data):\n",
    "    user_dict = create_user_dict(user_dict, source_common_data)\n",
    "else:\n",
    "    print(\"error!!!!!!\")\n",
    "    exit(0)\n",
    "\n",
    "source_total_data = read_dataset(f1 + \"new_reindex.txt\")\n",
    "target_total_data = read_dataset(f2 + \"new_reindex.txt\")\n",
    "\n",
    "source_user = create_user_dict(user_dict, source_total_data) # re-index\n",
    "target_user = create_user_dict(user_dict, target_total_data)\n",
    "\n",
    "source_item = create_item_dict(source_item, source_total_data)\n",
    "target_item = create_item_dict(target_item, target_total_data)\n",
    "\n",
    "print(len(source_user))\n",
    "print(len(target_user))\n",
    "print(len(source_item))\n",
    "print(len(target_item))\n",
    "\n",
    "generate_train_valid_test(f1, source_total_data, source_common_data, source_user, source_item, 0)\n",
    "generate_train_valid_test(f2, target_total_data, target_common_data, target_user, target_item, 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDRIB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
